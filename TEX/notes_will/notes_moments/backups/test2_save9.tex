\documentclass{article}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{url} 
\usepackage[authoryear]{natbib}
%%%\usepackage{babel}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{color}
\usepackage{framed}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
%%%\usepackage{custom_tex}
\usepackage{grffile}
\usepackage{multirow}
\usepackage{xcolor}
%\usepackage{setspace}
\usepackage{comment}
\usepackage{xr}
%\externaldocument{sharp_PCA_AoS_supp}

\newtheorem{thm}{Theorem}[section]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{example}[thm]{Example}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\theoremstyle{thm}
\newtheorem{conj}{Conjecture}
\theoremstyle{definition}
\newtheorem{rmk}{Remark}


\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\ep}{\varepsilon}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\GCD}{\mathbf{GCD}}
\newcommand{\XX}{\mathcal{X}}

\newcommand{\mean}[2]{\frac{1}{#1}\sum_{#2=1}^{#1}}


%
%
%
%
\begin{document}

\section{Setup}

We have the following model. $x$ is a signal of length $L$. For large $n$, we observe the vector $Y \in \mathbb{R}^n$, which is of the form 
%
\begin{align}
%
    Y = x \ast g + \ep
%
\end{align}
%
where $g$ is a sum of diracs at locations at least $L$ apart, and $\ep$ is a vector of $n$ iid Gaussians. The question is, can we recover $x$? 

We will assume that the first and last copies of $x$ is at least $L$ places removed from the boundaries of the interval $[1,n + L]$ (if not we can just zero-pad the interval). We will also denote by $I_1,\dots,I_J$ the $J$ subintervals containing the signal; and take $I_j = [a_j,b_j]$. When we take the limit $n \to \infty$, we'll let $J = J_n$ grow with $n$, as is natural. We will see that we require $J_n = \Omega(n)$ in order for the limits to not vanish (and obviously $J_n = O(n)$ too).

Note that I'm not just computing the expected value. I'm actually showing convergence almost surely as $n \to \infty$.


%
%
%

\section{First moment}

We'll compute the limiting vector of the sample average of all the windows. We fix an index $\ell$ between $1$ and $L$. The $\ell^{th}$ entry of the $i^{th}$ window is $Y_{i+\ell-1}$, $i=1,\dots,n$. Then the sample average of the $\ell^{th}$ entry is:
%
\begin{align}
%
    \frac{1}{n} \sum_{i=1}^n Y_{i+\ell - 1} &=
    \frac{1}{n}  \left( \sum_{i \notin I_j \forall j} \ep_i 
                  + \sum_{j=1}^J \sum_{k=1}^L (x[k] + \ep_{a_j + k-1} ) \right) 
    \nonumber \\
%
        &= \frac{1}{n}\sum_{i=1}^{n} \ep_i 
                  + \frac{J \cdot L}{n} \left(\frac{1}{L} \sum_{k=1}^L x[k]\right) 
    \nonumber \\
        &= \frac{1}{n}\sum_{i=1}^{n-L+1} \ep_i 
                  + \frac{J \cdot L}{n} \bar{x}       
     \nonumber \\
%
        &\operatorname*{\longrightarrow}^{a.s.}  \gamma  \cdot \bar{x}
%
\end{align}
%
where $\gamma = \lim_{n\to\infty} J_n L / n$; $\gamma$ is the fraction of the observations containing the signal. The limit of the $\ell^{th}$ entry of the mean does not depend on the index $\ell$, and is the same limit as if there were no noise at all, i.e.\ the noise completely averages out.



\begin{comment}

Actually, what Tamir meant when he said first moment was the average of the windows' averages (a scalar), not the average window (a vector). But this also converges a.s.\ to $\gamma \cdot \bar{x}$, by simply exchanging limits and finite sums, as follows. The sample mean of the window starting at index $i$ is just $\frac{1}{L} \sum_{k=0}^{L-1} Y_{i+k}$. Averaging these over all $n$ windows and taking the limit as $n \to \infty$ we get:
%
\begin{align}
%
    \lim_{n\to\infty} \frac{1}{n} \sum_{i=1}^n \frac{1}{L} \sum_{k=0}^{L-1} Y_{i+k}
    =  \frac{1}{L} \sum_{k=0}^{L-1} \lim_{n\to\infty} \frac{1}{n} \sum_{i=1}^n Y_{i+k}
    = \gamma \cdot \bar{x}.
%
\end{align}

\end{comment}

%
%
%

\section{Second moment}

We fix a value $\Delta$ between $0$ and $L-1$. We will compute the second moment:
%
\begin{align}
%
    M_2(\Delta) = 
        \lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^{n-\Delta} Y_i Y_{i+\Delta}.
%
\end{align}


%
%
%

\subsection{Clean signal without noise}

First, if there is no noise, and under the wide spacing assumption, we can break the sum into $J_n$ different sums, one for each copy of $x$ embedded in the signal:
%
\begin{align}
%
    M_2(\Delta) 
        &= \frac{1}{n} \sum_{j=1}^{J_n} \sum_{i=a_j}^{b_j-\Delta} Y_i Y_{i+\Delta}
        = \frac{1}{n} \sum_{j=1}^{J_n} \sum_{i=1}^{L-\Delta} x_i x_{i+\Delta}
    \nonumber \\
    &= \frac{J_n L}{n} \frac{1}{L} \sum_{i=1}^{L-\Delta} x_i x_{i+\Delta}
        \to \gamma \cdot R_2(\Delta),
%
\end{align}
%
where here we have defined $R_2(\Delta)$ as element $\Delta$ of the autocorrelation of $x$, and $\gamma = \lim_{n\to\infty} J_n L / n$ is the fraction of the observations containing the signal.

%
%
%

\subsection{Pure noise without signal}

Here, $Y_i = \ep_i$. We first fix $\Delta$ between $1$ and $L-1$. Break up the sum into $\Delta$ terms as follows:
%
\begin{align}
%
    M_2(\Delta) &=\frac{1}{n} \sum_{i=1}^n \ep_i  \ep_{i+\Delta}
        \nonumber \\
    &= \frac{1}{\Delta} \sum_{m=0}^{\Delta-1} \frac{1}{n/\Delta}\sum_{j=1}^{n/\Delta} 
        \ep_{j\Delta + m} \ep_{(j + 1)\Delta + m}.
%
\end{align}
%
Each term
%
\begin{math}
    \frac{1}{n/\Delta}\sum_{j=1}^{n/\Delta} 
        \ep_{j\Delta + m} \ep_{(j + 1)\Delta + m}
\end{math}
%
is an average of $n / \Delta$ iid terms with expectation zero, and so converges to 0 a.s.\ as $n \to \infty$. (Of course, it's not exactly $n / \Delta$ terms; there will be finitely many terms unaccounted for this way, but these are negligible).

If $\Delta = 0$, then the computation is even easier:
%
\begin{align}
%
    M_2(\Delta) = \frac{1}{n} \sum_{i=1}^n \ep_i^2 
        \operatorname*{\longrightarrow}^{a.s.} \sigma^2.
%
\end{align}

So $M_2(\Delta) = \sigma^2$ if $\Delta = 0$, and 0 otherwise.

%%%So the second moment matrix of pure noise converges a.s.\ as $n \to \infty$ to $\sigma^2 I_L$.


%
%
%

\subsection{Signal plus noise}

This part is now easy. Denote by $\XX = x \ast g$, so $Y = \XX + \ep$. Then the second moment of the signal plus noise is:
%
\begin{align}
%
    M_2(\Delta,Y) = M_2(\Delta,\XX) + M_2(\Delta,\ep) 
        + \frac{1}{n}\sum_{i=1}^{n-\Delta} \XX_i \ep_{i+\Delta}
        + \frac{1}{n}\sum_{i=1}^{n-\Delta} \XX_{i+\Delta} \ep_i.
%
\end{align}
%
The law of large numbers says the cross terms vanish as $n \to \infty$. So the limit is just:
%
\begin{align}
%
    M_2(\Delta,Y) = 
    \begin{cases}
        R_2(\Delta,X), &\text{ if } \Delta > 0; \\
        R_2(\Delta,X) + \sigma^2, &\text{ if } \Delta = 0.
    \end{cases}
%
\end{align}



\end{document}


\subsection{Second moment of signal plus noise}



Break the sum $\sum_{i=1}^n Y[i + k-1] Y[i + l - 1]$ into three terms: where $i + k-1$ and $i+l-1$ are disjoint from all intervals $I_j$ -- we will denote this set $\I$; where both $i + k - 1$ and $i + l -1$ lie entirely inside one of the $I_j$; and the rest, where exactly one of $i+k-1$ and $i+l-1$ lie in an interval $I_j$. So we have:
%
\begin{align}
%
    & \sum_{i=1}^n Y[i+k-1] Y[i+l-1]    \nonumber \\
    =& \left(\sum_{i \in \I}  
        + \sum_{j=1}^J \sum_{i=a_j - l + 1}^{a_j - k} 
        + \sum_{j=1}^J \sum_{i=b_j - l + 2}^{b_j - k+1} 
        + \sum_{j=1}^J \sum_{i=a_j-k+1}^{b_j-l+1}
      \right)  Y[i+k-1] Y[i+l-1]
%
\end{align}

We will use that $Y[a_j + m] = x[m+1] + \ep[a_j+m]$, $m=0,\dots,L-1$. For the second term, the inner sum is
%
\begin{align}
%
    &\sum_{i=a_j - l + 1}^{a_j - k} Y[i+k-1] Y[i+l-1]
        \nonumber \\
%
    =&\sum_{i=1}^{\Delta} Y[a_j -\Delta + i-1] Y[a_j + i-1]
        \nonumber \\
%
    =& \sum_{i=1}^{\Delta}\ep[a_j -\Delta + i-1] (x[i] + \ep[a_j + i-1])
        \nonumber \\
%
    =& \sum_{i=1}^{\Delta}\ep[a_j -\Delta + i-1] x[i]
        + \sum_{i=1}^{\Delta}\ep[a_j -\Delta + i-1] \ep[a_j + i-1]
%
\end{align}
%

We also use that $Y[b_j - m] = x[L-m] + \ep[b_j-m]$, $m=0,\dots,L-1$. For the third term, the inner sum is
%
\begin{align}
%
    & \sum_{i=b_j - l + 2}^{b_j - k+1} Y[i+k-1] Y[i+l-1]
        \nonumber \\
%
    =& \sum_{i=1}^{\Delta} Y[i+b_j-\Delta] Y[i+b_j]
        \nonumber \\
%
    =& \sum_{i=1}^{\Delta} (x[L-\Delta+i]+\ep[i+b_j-\Delta]) \ep[i+b_j]
        \nonumber \\
%
    =& \sum_{i=1}^{\Delta} x[L-\Delta+i] \ep[i+b_j]
        + \sum_{i=1}^{\Delta} \ep[i+b_j-\Delta] \ep[i+b_j]
\end{align}
%

The inner sum of the fourth term is:
%
\begin{align}
%
    &\sum_{i=a_j-k+1}^{b_j-l+1} Y[i+k-1] Y[i+l-1]
        \nonumber \\
%
    =& \sum_{i=1}^{L - \Delta} Y[i+a_j-1] Y[i+a_j + \Delta - 1]
        \nonumber \\
%
    =& \sum_{i=1}^{L - \Delta} (x[i]+\ep[i+a_j-1])(x[i+\Delta] +\ep[i+a_j + \Delta - 1])
        \nonumber \\
%
    =& \sum_{i=1}^{L - \Delta} x[i] x[i+\Delta]
        + \sum_{i=1}^{L - \Delta} x[i]\ep[i+a_j + \Delta - 1]
        \nonumber \\
        &+ \sum_{i=1}^{L - \Delta} \ep[i+a_j-1] x[i+\Delta] 
        + \sum_{i=1}^{L - \Delta} \ep[i+a_j-1]\ep[i+a_j + \Delta - 1])
%
\end{align}


\end{document}
