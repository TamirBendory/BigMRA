\documentclass{article}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{url} 
\usepackage[authoryear]{natbib}
%%%\usepackage{babel}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{color}
\usepackage{framed}
\usepackage{lscape}
\usepackage{rotating}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
%%%\usepackage{custom_tex}
\usepackage{grffile}
\usepackage{multirow}
\usepackage{xcolor}
%\usepackage{setspace}
\usepackage{comment}
\usepackage{xr}
%\externaldocument{sharp_PCA_AoS_supp}

\newtheorem{thm}{Theorem}[section]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{example}[thm]{Example}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{prop}[thm]{Proposition}
\theoremstyle{thm}
\newtheorem{conj}{Conjecture}
\theoremstyle{definition}
\newtheorem{rmk}{Remark}


\newcommand{\F}{\mathcal{F}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\ep}{\varepsilon}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\GCD}{\mathbf{GCD}}

\newcommand{\mean}[2]{\frac{1}{#1}\sum_{#2=1}^{#1}}


%
%
%
%
\begin{document}

\section{Setup}

We have the following model. $x$ is a signal of length $L$. For large $n$, we observe the vector $Y \in \mathbb{R}^n$, which is of the form 
%
\begin{align}
%
    Y = x \ast g + \ep
%
\end{align}
%
where $g$ is a sum of diracs at locations at least $L$ apart, and $\ep$ is a vector of $n$ iid Gaussians. The question is, can we recover $x$? 

The approach Tamir proposed was the following. We look at all length $L$ subvectors of $Y$ (with consecutive elements). Compute the mean, power spectrum, and bispectrum for each, and average. This should hopefully converge in the large $n$ limit to some function of $x$. Then attempt to invert this function, perhaps by least squares.

One of Tamir's concerns is that the noise terms are not independent between overlapping windows. I'll show that at least for the first and second moments, things are okay as long as there are $\Omega(n)$ copies of $x$ buried in $Y$. The bispectrum is probably fine too but I haven't written it out yet.


We will assume that the first and last copies of $x$ is at least $L$ places removed from the boundaries of the interval $[1,n + L]$ (if not we can just zero-pad the interval). We will also denote by $I_1,\dots,I_J$ the $J$ subintervals containing the signal; and take $I_j = [a_j,b_j]$. When we take the limit $n \to \infty$, we'll let $J = J_n$ grow with $n$, as is natural. We will see that we require $J_n = \Omega(n)$ in order for the limits to not vanish.

Note that I'm not just computing the expected value. I'm actually showing convergence almost surely as $n \to \infty$.



\section{First moment}

We'll compute the limiting vector of the sample average of all the windows. We fix an index $\ell$ between $1$ and $L$. The $\ell^{th}$ entry of the $i^{th}$ window is $Y_{i+\ell-1}$, $i=1,\dots,n$. Then the sample average of the $\ell^{th}$ entry is:
%
\begin{align}
%
    \frac{1}{n} \sum_{i=1}^n Y_{i+\ell - 1} &=
    \frac{1}{n}  \left( \sum_{i \notin I_j \forall j} \ep_i 
                  + \sum_{j=1}^J \sum_{k=1}^L (x[k] + \ep_{a_j + k-1} ) \right) 
    \nonumber \\
%
        &= \frac{1}{n}\sum_{i=1}^{n} \ep_i 
                  + \frac{J \cdot L}{n} \left(\frac{1}{L} \sum_{k=1}^L x[k]\right) 
    \nonumber \\
        &= \frac{1}{n}\sum_{i=1}^{n-L+1} \ep_i 
                  + \frac{J \cdot L}{n} \bar{x}       
     \nonumber \\
%
        &\operatorname*{\longrightarrow}^{a.s.}  \gamma  \cdot \bar{x}
%
\end{align}
%
where $\gamma = \lim_{n\to\infty} J_n L / n$; $\gamma$ is the fraction of the observations containing the signal. The limit of the $\ell^{th}$ entry of the mean does not depend on the index $\ell$, and is the same limit as if there were no noise at all, i.e.\ the noise completely averages out.

Actually, what Tamir meant when he said first moment was the average of the windows' averages (a scalar), not the average window (a vector). But this also converges a.s.\ to $\gamma \cdot \bar{x}$, by simply exchanging limits and finite sums, as follows. The sample mean of the window starting at index $i$ is just $\frac{1}{L} \sum_{k=0}^{L-1} Y_{i+k}$. Averaging these over all $n$ windows and taking the limit as $n \to \infty$ we get:
%
\begin{align}
%
    \lim_{n\to\infty} \frac{1}{n} \sum_{i=1}^n \frac{1}{L} \sum_{k=0}^{L-1} Y_{i+k}
    =  \frac{1}{L} \sum_{k=0}^{L-1} \lim_{n\to\infty} \frac{1}{n} \sum_{i=1}^n Y_{i+k}
    = \gamma \cdot \bar{x}.
%
\end{align}



\section{Second moment}

First we'll compute the limit in the case of pure noise; then we'll use this for signal plus noise.


\subsection{Second moment of noise}

Here, $Y_i = \ep_i$. We fix two indices $k$ and $l$, and we suppose $k < l$. We will let $\Delta = l-k > 0$. We want to compute the limit $\frac{1}{n} \sum_{i=1}^n Y_{i + k - 1} Y_{i + l - 1} = \frac{1}{n} \sum_{i=1}^n Y_{i + k - 1} Y_{i + k + \Delta - 1}$. Break up the sum into $\Delta$ terms as follows:
%
\begin{align}
%
    &\frac{1}{n} \sum_{i=1}^n Y[i + k - 1]  Y[i + k+\Delta - 1]   
    \nonumber \\
    =& \frac{1}{\Delta} \sum_{m=0}^{\Delta-1} \frac{1}{n/\Delta}\sum_{j=1}^{n/\Delta} 
                      Y[j\Delta + m] Y[(j + 1)\Delta + m].
%
\end{align}
%
Each term
%
\begin{math}
    \frac{1}{n/\Delta}\sum_{j=1}^{n/\Delta} 
                      Y[j\Delta + m] Y[(j + 1)\Delta + m]
\end{math}
%
is an average of $n / \Delta$ iid terms with expectation zero, and so converges to 0 a.s.\ as $n \to \infty$. (Of course, it's not exactly $n / \Delta$ terms; there will be finitely many terms unaccounted for this way, but these are negligible).


If $k = l$, then it's even easier:
%
\begin{align}
%
    \frac{1}{n} \sum_{i=1}^n Y[i + k - 1]^2 \operatorname*{\longrightarrow}^{a.s.} \sigma^2.
%
\end{align}

So the second moment matrix of pure noise converges a.s.\ as $n \to \infty$ to $\sigma^2 I_L$.


\subsection{Second moment of signal plus noise}



Break the sum $\sum_{i=1}^n Y[i + k-1] Y[i + l - 1]$ into three terms: where $i + k-1$ and $i+l-1$ are disjoint from all intervals $I_j$ -- we will denote this set $\I$; where both $i + k - 1$ and $i + l -1$ lie entirely inside one of the $I_j$; and the rest, where exactly one of $i+k-1$ and $i+l-1$ lie in an interval $I_j$. So we have:
%
\begin{align}
%
    & \sum_{i=1}^n Y[i+k-1] Y[i+l-1]    \nonumber \\
    =& \left(\sum_{i \in \I}  
        + \sum_{j=1}^J \sum_{i=a_j - l + 1}^{a_j - k} 
        + \sum_{j=1}^J \sum_{i=b_j - l + 2}^{b_j - k+1} 
        + \sum_{j=1}^J \sum_{i=a_j-k+1}^{b_j-l+1}
      \right)  Y[i+k-1] Y[i+l-1]
%
\end{align}

We will use that $Y[a_j + m] = x[m+1] + \ep[a_j+m]$, $m=0,\dots,L-1$. For the second term, the inner sum is
%
\begin{align}
%
    &\sum_{i=a_j - l + 1}^{a_j - k} Y[i+k-1] Y[i+l-1]
        \nonumber \\
%
    =&\sum_{i=1}^{\Delta} Y[a_j -\Delta + i-1] Y[a_j + i-1]
        \nonumber \\
%
    =& \sum_{i=1}^{\Delta}\ep[a_j -\Delta + i-1] (x[i] + \ep[a_j + i-1])
        \nonumber \\
%
    =& \sum_{i=1}^{\Delta}\ep[a_j -\Delta + i-1] x[i]
        + \sum_{i=1}^{\Delta}\ep[a_j -\Delta + i-1] \ep[a_j + i-1]
%
\end{align}
%





\end{document}
