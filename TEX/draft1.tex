\documentclass[english,11pt]{article}

\pdfoutput=1

\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{verbatim}
\usepackage{float}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
%\usepackage{multirow}
\usepackage{color}
\usepackage{url}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathtools} 
\usepackage[margin=1.2in]{geometry}

\newcommand{\TODO}[1]{{\color{red}{[#1]}}}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
%\numberwithin{figure}{section}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}[section]
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\theoremstyle{remark}
\newtheorem{claim}[thm]{\protect\claimname}
\theoremstyle{plain}
\newtheorem{lem}[thm]{\protect\lemmaname}

\newtheorem*{lem*}{Lemma}

\theoremstyle{remark}
\newtheorem{rem}[thm]{\protect\remarkname}
\theoremstyle{plain}
\newtheorem{corollary}[thm]{\protect\corollaryname}
\theoremstyle{plain}
\newtheorem{proposition}[thm]{\protect\propositionname}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\usepackage{slashbox}

\usepackage{babel}
\providecommand{\claimname}{Claim}
\providecommand{\definitionname}{Definition}
\providecommand{\lemmaname}{Lemma}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}
\providecommand{\corollaryname}{Corollary}
\providecommand{\propositionname}{Proposition}


\newcommand{\reals}{\mathbb{R}}
\newcommand{\RL}{\mathbb{R}^L}
\newcommand{\CL}{\mathbb{C}^L}
\newcommand{\RN}{\mathbb{R}^N}
\newcommand{\RNN}{\mathbb{R}^{N\times N}}
\newcommand{\CNN}{\mathbb{C}^{N\times N}}
\newcommand{\inner}[1]{\left\langle {#1} \right\rangle}
\newcommand{\hx}{\hat{x}} 
\newcommand{\one}{\mathbf{1}} 
\newcommand{\SNR}{{\textsf{SNR}}} 

\begin{document}

\title{Estimation below the detection limit}


\author{Tamir Bendory}
\maketitle

\begin{abstract}
	Here comes the abstract
\end{abstract}

\section{Introduction}

In this paper, we consider the problem of estimating a set of signals $x_1,\ldots,x_K$ from their multiple occurrences in unknown, random, locations in a data sequence $y$. The data may also contain background information -- independent of the signals -- which is modeled as noise.
For one-dimensional signals, the data can be thought of as a long time series and the signals as repetitive short events. For two-dimensional signals, $y$ presents a big image, containing many smaller images.  
The problem is then to estimate the signals $x_1,\ldots x_K$ from $y$. This
model appears, in different noise levels, in many applications, including spike sorting~\cite{lewicki1998review}, passive radar~\cite{gogineni2017passive} and system identification~\cite{ljung1998system}.
In Section~\ref{sec:model} we provide a precise mathematical formulation of the model and the estimation problem.

If the noise level is negligible, estimating the signals is easy. 
In this scenario, standard detection and clustering algorithms can produce multiple copies of each signal that can be then averaged. 
Even in higher noise level regimes, clever methods based on template matching, such as those used in structural biology~\cite{heimowitz2018apple} and radar~\cite{gogineni2017passive}, may work. \TODO{Do we want to add another 1D example to demonstare the problem, similarly to Figure 1.1 in the bispectrum paper?}
However, in the low signal--to--noise (\SNR) regime, detection of individual signal occurrences is impossible as explained in Section~\ref{sec:model}. Figure~\ref{fig:example} illustrates the problem in different noise levels and one underlying signal ($K=1$). 
Figure~\ref{fig:signal2D_clean} shows a $21\times 21$ image, which is a downsampled version of a projection of TKTK, taken from \textsf{ASPIRE} package~\cite{aspire} \TODO{The image is taken from the example folder}. 
Figure~\ref{fig:data2D_clean} shows a excerpt of a big image (the data image) that contains many repetitions of the projection. In Figures~\ref{fig:data2D_noisy_02} and~\ref{fig:data2D_noisy_1}, the same excerpt is shown with the addition of i.i.d.\ Gaussian noise with standard deviations of $\sigma=0.2$ and $\sigma=1$, respectively.  Figures~\ref{fig:signal2D_LS} and~\ref{fig:signal2D_LS_inter} show estimates of the projection from noisy data with $\sigma=1$. These examples demonstrate that our method can work even if the data may seem as a pure noise. In Section~\ref{sec:numerics} we provide the details of this experiment and show more corroborating experiments. 

In this work we focus on the low $\SNR$ regime. In order to estimate the signal, we use autocorrelation analysis of the data. 
In a nutshell, the method consists of two stages. First, we estimate a mix of the low--order autocorrelation functions of the signals from the data. These quantities can be estimated, to any desired accuracy, if the signals appear enough times in the measurement, without the need to detect individual occurrences. Then, the signals are estimated from the mixed autocorrelations using a non-convex least-squares (LS).
In Section~\ref{sec:autocorrelation}, we elaborate on the technique and prove some of its properties.
Interestingly, expectation-maximization (EM) --  a popular framework for similar estimation problems, such as Gaussian mixture models and multireference alignment -- is intractable for this problem. Even if the number of signal occurrences $M$ is known and $K=1$, at each iteration 
EM needs to assign probabilities to each possible combination of  $M$ copies of the current signal estimation in the measurement, which is of order of $O(N^M)$.



\begin{figure}

\advance\leftskip-2cm
\advance\rightskip -2cm

%	\centering
\begin{subfigure}{.6\textwidth}
	\centering
	\includegraphics[scale=0.6]{data2D_clean}
	\vspace*{-20pt}
\caption{}
\label{fig:data2D_clean}
	\end{subfigure}%
	\begin{subfigure}{.6\textwidth}
	\centering
	\includegraphics[scale=0.6]{signal2D_clean}
	\vspace*{-20pt}
	\caption{}
	\label{fig:signal2D_clean}
\end{subfigure}%


	\begin{subfigure}{.6\textwidth}
	\centering
	\includegraphics[scale=0.6]{data2D_noisy_02}
	\vspace*{-20pt}
	\caption{}
	\label{fig:data2D_noisy_02}
\end{subfigure}
\begin{subfigure}{.6\textwidth}
	\centering
	\includegraphics[scale=0.6]{signal2D_LS_all_images}
	\vspace*{-20pt}
	\caption{}
	\label{fig:signal2D_LS}
\end{subfigure}


\begin{subfigure}{.6\textwidth}
\centering
\includegraphics[scale=0.6]{data2D_noisy_1}
\vspace*{-20pt}
\caption{}
\label{fig:data2D_noisy_1}
\end{subfigure}
\begin{subfigure}{.6\textwidth}
	\centering
	\includegraphics[scale=0.6]{signal2D_LS_intermediate}
%	\vspace*{-20pt}
	\caption{}
	\label{fig:signal2D_LS_inter}
\end{subfigure}

\caption{Figure (a) shows an excerpt of 2D data with multiple signal occurrences and no noise. Each small image is the $21\times 21$ image shown in Figure (b). Figures (c) and (e) show the same data, now contaminated with i.i.d.\ Gaussian noise with $\sigma=0.2$ and $\sigma=1$, respectively. Figures (d) and (f) show estimates of the image (b) from data with noise level $\sigma=1$ (as in Figure (e)). The signal appeared $M = 561\times 10^6$ and $M = 110\times 10^6$ times and the normalized recovery error is  $0.078$ and $0.135$, respectively.}
%	we used $71\times10^4$
% (d) 710K micrographs, $M=561Mega$, error  =  (e)The same data as in Figure~\ref{fig:signal2D_clean} contaminated with i.i.d./ Gaussian noise with $\sigma=0.2$ (f) Recovery with 140K micrographs and $M = 110Mega$, error = 0.135}
\label{fig:example}
\end{figure}



This work is primely motivated by cryo-electron microscopy (cryo--EM), which is an innovative  single particle reconstruction technology. The acquired data in a cryo-EM experiment is contaminated with high noise levels. Therefore, any molecule reconstruction algorithm must take  the challenging $\SNR$ level into account.  In the last part of this manuscript, we draw connections with the estimation problem under consideration and the cryo--EM problem.


\section{Model}  \label{sec:model}

Let $x_1,\ldots,x_K\in\RL$ be the sought signals and let $y\in\RN$ be the data. For each $x_i$, we associate a binary signal  $s_i\in\{0,1\}^N$, referred to as the \emph{support signal} and let $s = \sum_{i=1}^Ks_i$. The nonzero values of $s_i$ indicates the locations of $x_i$ in $y$. If $s_i[n]=1$, then $y[n+j] = x_i[n+j]+\varepsilon[n+j]$ for $j=0,\ldots,L-1$.
We denote the cardinality of $s_i$ by $M_i$  and $M = \sum_{i=1}^{K}M_i$. Neither the $M_i$'s nor $M$ is assumed to be known.


The support signals are generated by the following generative model. The signal $s$ is initialized with zeros.
First, an index $i_1$ is drawn uniformly from $\{1,\ldots,N\}$ and we set $s[i_1]=1$. A second index $i_2$ is then drawn uniformly from $\{1,\ldots,N\}$. If $\vert i_2-i_1\vert \geq L$,
then we set $s[i_2]=1$, otherwise we keep $s[i_2]=0$ and draw a new index  from uniform distribution. We then proceed adding  nonzero entries to $s$,  while keeping $L$  entries separation until some halting criterion is obtained. 
We note that if the support is sparse enough, this generative model can be approximated by a simple Bernoulli process which takes the values of one and zero with probabilities  $M/N$ and $1-M/N$, respectively.
Once $s$ is determined, each one of its nonzero entries is associated with one of the $s_i$'s. 
In particular, for each $s[k]\neq 0$ we set $s_i[k] = 1$ and $s_j[k] = 0$ for $i\neq j$, where $i$ is drawn form an unknown probability over  $\{1,\ldots,K\}$. 

The simplest way to present the forward model is a mix of blind deconvolution problems between the support signals and the target signals
\begin{equation}
y = \sum_{i=1}^K x_i\ast s_i + \varepsilon,\quad \varepsilon\sim\mathcal{N}(0,\sigma^2 I).
\end{equation}
We model the background information as i.i.d.\ Gaussian noise with zero mean and $\sigma^2$ variance. 
The goal is to estimate $x_1,\ldots,x_K$ from $y$.

Blind deconvolution is a longstanding problem, arising in a variety of engineering and scientific applications, such as astronomy, communication, image deblurring, system identification and optics; see~\cite{jefferies1993restoration,shalvi1990new,ayers1988iterative,abed1997blind}, just to name a few. Clearly, the problem is ill-posed without additional information. In our case, the prior information is that $s$ is a binary, sparse, signal. 
Other settings of blind deconvolution problems have been analyzed recently under different settings, see for instance~\cite{ahmed2014blind,li2016identifiability,li2016rapid,ling2015self,ling2017blind,chi2016guaranteed}
where the focus is on high $\SNR$ regimes.


An important feature of the problem under consideration is that while both $x_i$'s and $s_i$'s are unknown, the goal is merely to estimate the $x_i$'s. The  $s_i$'s  are referred to as \emph{nuisance  variables}. Indeed, in many blind deconvolution applications the goal is merely to estimate one of the unknown signals. For instance, in image deblurring, both the blurring
kernel and the high-resolution image are unknown, but the prime goal is only
to sharpen the image.
If $x$ is known and $K=1$, then  $s$ can be estimated by linear programming  in the high $\SNR$ regime~\cite{de2012exact,duval2015exact,bendory2016robust,bendory2017robust,bernstein2017deconvolution}. However, in the low $\SNR$ regime, estimating $s$ is impossible. To see that, suppose that an oracle provides us $M$ windows of length $W>L$, each contains one copy of $x$. Put it differently, we get a series of windows, each one contains a signal at unknown location. 
Estimating the first entry of the signal within each  window is an easier problem than detecting the support of $s$. However, even this problem -- called alignment or synchronization -- is impossible in the low $\SNR$ regime.
For instance, the variance of any estimator is, at best, proportional to $\sigma^2$ and independent of the number of windows, even if $x$ is known~\cite{aguerrebere2016fundamental}. Therefore, we conclude that that detecting the nonzero values of $s$ is impossible in low \SNR. 
In the next section we show that if $M$ is large enough, then estimating the signals is possible, in any $\SNR$ level and to any accuracy, although we cannot detect their occurrences in $y$. 



\section{Autocorrelation analysis}   \label{sec:autocorrelation}

Our method for estimating the signals is a two-stages procedure. 
First, we use the autocorrelation functions of the data to estimate a mix (i.e., linear combination) of the $K$ signal's autocorrelations. The mixed autocorrelation can be estimated to any accuracy, in any $\SNR$ level, if $M$ is large enough and the separation condition on the support is met. Then, we  use a nonconvex LS  to estimate the signals from their mixed autocorrelation. 
The precise recovery procedure we are using, based on nonconvex optimization, will be elaborated in the next section.


\subsection{Aperiodic autocorrelation functions} \label{sec:aperiodic_ac}

For the purpose of this paper, we need the first three (aperiodic) autocorrelation functions. The first-order autocorelation is the mean of the signals. For  
$z\in\RL$ and $k\geq 2$, the higher order autocorrelations are defined by
\begin{equation*}
a_z^k[\ell_1,\ldots,\ell_{k-1}] =  \sum_{i=0}^{L-1-\max\{\ell_1,\ldots,\ell_{k-1}\}} z[i]z[i+\ell_1]\ldots z[i+\ell_{k-1}].
\end{equation*}
Explicitly, the first three autocorrelations are
\begin{align} \label{eq:autocorrelations}
a_z^1 &= \sum_{i=0}^{L-1} z[i], \nonumber\\
a_z^2[\ell] &= \sum_{i=0}^{L-1-\ell} z[i]z[i+\ell], \nonumber\\
a_z^3[\ell_1,\ell_2] &= \sum_{i=0}^{L-1-\max\{\ell_1,\ell_2\}} z[i]z[i+\ell_1]z[i+\ell_2]. 
\end{align}
Note that the autocorrelation functions are symmetric so that $a_z^2[\ell] = a_z^2[-\ell]$ and $a_z^3[\ell_1,\ell_2] = a_z^3[-\ell_1,-\ell_2]$. 
Additionally, if the moments of the signal depend only on the difference between the indices (Toeplitz structure), the they are equivalent to the autocorrelation functions.

A one-dimensional signal is determined uniquely by its third-order auto-correlation:
\begin{proposition} \label{prop:uniqueness}
	Let $z\in\RL$ and suppose that $z[0]$ and $z[L-1]$ are nonzeros. Then, $z$  is determined uniquely from  $a_z^2$ and $a_z^3$.
	\begin{proof}
		From the second-order autocorrelation, we ge
		\begin{equation*}
		a_z^2[L-1] = z[0]z[L-1]\neq 0.
		\end{equation*}
		Then, for all $k=0,\ldots L-1$,  we can compute $z[k]$  from the third-order autocorrelation
		\begin{eqnarray}
		a_z^3[k,L-1] = z[0]z[k]z[L-1].
		\end{eqnarray}
	\end{proof}
\end{proposition}
\noindent This result can be extended to any dimension.
We note that the length of the signal can be easily derived from the length of the second-order autocorrelation of the signal. Therefore, the assumption that $z[0]$ and $z[L-1]$ are nonvanishing is met in practice. We also note that for one-dimensional signals, the second-order autocorrelation does not determined the signals uniquely~\cite{beinert2015ambiguities,bendory2017fourier}. For dimensions greater than one, almost all signals are determined uniquely from their aperiodic autocorrelations, up to sign (phase in the complex case) and reflection through the origin (with conjugation in the complex case)~\cite{hayes1982reconstruction,hayes1982reducible}. The sign ambiguity can be resolved by the mean of the signal if it is not zero. However,  in order to determine the reflection symmetry, one needs to use additional information, such as higher-order statistics.

High-order autocorrelations have been used for a variety of tasks in signals processing, bla bla. In particular, \TODO{Here we should refer to Gianakis's paper}



The uniqueness for $K>1$ was explored  for the related case of  periodic autocorrelation functions. Recall that the $k$th periodic autocorrelation of $z\in\mathbb{R}^L$ is given by
\begin{equation*}
b_z^k[\ell_1,\ldots,\ell_{k-1}] =  \sum_{i=0}^{W-1} z[i]z[(i+\ell_1)\mod L]\ldots z[(i+\ell_{k-1})\mod L].
\end{equation*}
If $z[n]=0$ for $n=L/2,\ldots L-1$, then  $b_z^k[\ell_1,\ldots,\ell_{k-1}] = a_z^k[\ell_1,\ldots,\ell_{k-1}]$. 
In~\cite{bandeira2017estimation}, it was shown that a mix of $K$ third-order periodic autocorrelations determine  a finite list of $K$ generic signals if $L/6\gtrsim K$. Empirical evidences hints that this finite list includes only group symmetries~\cite{boumal2017heterogeneous}.

\subsection{Estimating autocorrelations from the data} \label{sec:estimating_ac}

In order to estimate the mixed autocorrelation of the $K$ signals, we first compute the first $L$ entries of the data's autocorrelations. For the purpose of the analysis, we consider  the asymptomatic regime where $M_1,\ldots,M_K,N\to\infty$. We define the ratio of the measurement occupied by each one of the signal by
\begin{equation}
\gamma_i = \frac{M_i L}{N},
\end{equation}
and $\gamma = \sum_{i=1}^K\gamma_i$. 
Under the spacing constraint, we have $\gamma\leq\frac{L}{2L-1}\approx 1/2$.

The main pillar of this work is the following simple observation.
If the support $s$ satisfies the spacing constraint of $L$ entries, then the first $L$ entries of the data autocorrelations converge to a mix of scaled versions of the signals autocorrelations:
\begin{align} \label{eq:data_ac}
\lim_{N\to\infty} a_y^1 &= \sum_{i=1}^K\gamma_i a_{x_i}^1, \nonumber\\
\lim_{N\to\infty} a_y^2[\ell] &= \sum_{i=1}^K\gamma_i a_{x_i}^2[\ell] +\sigma^2\delta[\ell],\\
\lim_{N\to\infty} a_y^3[\ell_1,\ell_2] &= \sum_{i=1}^K\gamma_i a_{x_i}^3[\ell_1,\ell_2] + \sigma^2\left(\sum_{i=1}^K\gamma_i a_{x_i}^1\right)(\delta[\ell_1,0]+\delta[0,\ell_2]+\delta[\ell_1,\ell_2]), \nonumber
\end{align}
for $\ell,\ell_1,\ell_2=0,\ldots L-1$, and where $\delta$ denotes the Kronecker delta function. 
These relations are proven in Appendix~\ref{sec:autocorrelation_computation}. The analysis is similar to~\cite{bendory2017bispectrum,boumal2017heterogeneous}, yet a particular caution should be taken with the statistical dependencies of the noise entries. 
The relations~\eqref{eq:data_ac} imply that for $K=1$ and given $M$ and $\sigma$, one can estimate the signal for any noise level if $M$ is large enough. Next, we show that for $K=1$,  $M$ and $\sigma$ are also uniquely determined from the autocorrelations. 

An interesting consequence of our analysis is that, at least for $K=1$, the third autocorrelation of the data suffices to determine the signal. 
The minimal order of data statistics that we use to get an accurate estimation of a signal is important to understand, in the asymptotic $\SNR$ regime, the sample complexity of the problem.
In methods which are based on detection and averaging, the number of signals occurrences  scales like $\sigma^2$. Taking the $k$th order autocorrelation function amplifies the variance of the noise by a factor of $k$. Therefore, the required number signal occurrences should scale like $\sigma^{2k}$  to retain some constant estimation error. Accordingly, in our  method, $M$ scales like $\sigma^6$. In Section~\ref{sec:numerics}, we show empirically the third-order autocorrelation suffices also for a mixture of $K>1$ signals without prior knowledge of the $M_i$'s and $\sigma$. 


%This means that given $M_1,\ldots,M_K$ and $\sigma^2$ and $K$ does not exceed the limit for uniqueness, the one can estimate the signals from the third-order autocorrelation of the data. In other words, the signals can be estimated in asymptotic estimation rate of $\sigma^6/N$.


If the noise level $\sigma^2$ is known and  $K=1$, one can estimate $M/N$ from only the first two moments.
\begin{proposition} \label{prop:gamma}
	Let $K=1$, $N\to\infty$ and $\sigma$ fixed. Then, 
	\begin{equation*}
	\frac{M}{N} = \frac{1}{L}\frac{(a^1_y)^2}{\sum_{j=0}^{L-1}a_y^2[j]-\sigma^2}.
	\end{equation*}
	\begin{proof}
The proof follows plugging the explicit expressions of~\eqref{eq:data_ac} into the right hand side of the equality.
\end{proof}
\end{proposition}

If we use third-order autocorrelation information, then it possible to estimate both $M/N$ and $\sigma$ simultaneously. In contrast to Proposition~\ref{prop:gamma}, this result does not come with explicit formula.
\begin{proposition} \label{prop:gamma_sigma}
	Let $K=1$, $N\to\infty$ and $\sigma$ fixed. Then, $a_y^1,a_y^2$ and  $a_y^3$ determine $M/N$ and $\sigma$ uniquely for generic signal. If $\frac{M}{N}\geq\frac{1}{4(L-1)}$, then it holds for any nonzero $x$. 
	\begin{proof}
		See Appendix~\ref{sec:proof_prop_gamma_sigma}.
	\end{proof}
\end{proposition}

From Propositions~\ref{prop:uniqueness} and~\ref{prop:gamma_sigma} we can directly deduces the following.
\begin{corollary}
	For $K=1$, $N\to\infty$ and fixed $\sigma$, a generic signal obeying $x[0],x[L-1]\neq 0$, the ratio $M/N$ and $\sigma$ can be recovered from the first three autocorrelation functions. If  $\frac{M}{N}\geq\frac{1}{4(L-1)}$, then it holds for any nonzero $x$.
\end{corollary}


\section{Numerical experiments}   \label{sec:numerics}

\subsection{One-dimensional experiments}

Here will come the 1D experiment with heterogeneity

\subsection{Two-dimensional experiments}

In this section, we provide the details of the 2D experiment shown in Figure~\ref{fig:example}. In the experiment, we generated a sequence of $767\times 10^3$ 2D images (``micrograph") of size $2000\times 2000$. Each micrograph had around $\sim 900$ signal occurrences (depends on the stopping criterion that we need to explain) and in total $M = 561\times 10^6$. The noise level was $\sigma=1$.  

To accelerate the experiment, we estimated only the first two autocorrelations of the signal. As mentioned in Section~\ref{sec:estimating_ac}, this data determines a 2D signal uniquely, up to possible reflection symmetry. 
In this experiment, we chose the correct reflection manually. To estimate the signal we used the Relax-Reflect-Reflect (RRR) algorithm, which is a known phase retrieval algorithm. Recall that the second-order autocorrelation is equivalent to the power spectrum of the signal.
Starting form random initialization of size $(2L-1)\times(2L-1)$, its $k$th  iteration take the form of 
 \begin{equation*}
 x_k = x_{k-1} + \beta(P_2(2P_1(x_{k-1}) -x_{k-1}) - P_1(x_{k-1}), 
  \end{equation*}
 where $P_1$ is a projection that keeps the values of the  $L\times L$ entries in the upper-left corner and zeros out all other entries, $P_2$ is a projection that keeps the Fourier phase of $x_{k-1}$ and impose the correct Fourier magnitudes, and we set $\beta = 0.5$. The solution of the RRR scheme was used to initialized a LS estimation on the first two moments with signals of length $(2L-1)\times(2L-1)$ for the 200 iterations. Finally, we chose the $L\times L$ upper left corner and refined it using LS with 500 iterations. 
 
\section{Conclusion} 
Here we conclude the paper. Points to mention;
\begin{itemize}
\item cryo -- EM
\item  structured background 
\item   without separation   	
\item  sample complexity
\end{itemize}
	
%In particular, in cryo--EM, multiple projections of a molecule (signal), taken
%from unknown viewing directions, are recorded on two-dimensional detector
%array. Current algorithms try to detect these projections in a low SNR regime
%and then use them for the reconstruction process. Since the
%Then, the reconstruction problem is to detect these projection, and then use
%them to estimat
%The data acquired in both technologies is composed of multiple projections
%of a molecule (the signal), taken from unknown viewing direction, and drawn in
%high noise level. The reconstitution problem is then to estimate the molecule
%from this data.


\bibliographystyle{plain}
\bibliography{ref}



\appendix

\section{Autocorrelation estimations} \label{sec:autocorrelation_computation}
 
To analyze the asymptotic behavior of the data autocorrelation functions, we consider the case of one signal $K=1$. The extension to $K>1$ is straightforward by averaging the contributions of all signal with the appropriates weights, see~\cite{boumal2017heterogeneous}. 

Let us define
\begin{equation}
\gamma = \lim_{N\to\infty} \frac{M_NL}{N}<1.
\end{equation}
With a bit abuse of notation, $M_N$ means that $M$ is a function of $N$. Indeed, by assuming $M_N=\Omega(N)$, we deduce $\gamma>0$.
We start by considering the first autocorrelation of the data
\begin{equation}
a_y^1 = \sum_{i=0}^{N-1} y[i] = \frac{1}{N/L}\sum_{j=0}^{M_N-1}\frac{1}{L}\sum_{i=0}^{L-1}x[i] + \underbrace{\frac{1}{N}\sum_{i=0}^{N-1}\varepsilon[i]}_{\text{noise term}} \xrightarrow{a.s.}\gamma a_x^1,
\end{equation}
where the noise term converges to zero almost surely (a.s.) by the law of large numbers.

We proceed with the second autocorrelation for fixed $\ell\in[0,\ldots,L-1]$. We can compute:
\begin{equation}
\begin{split}
a_y^2[\ell] & = \frac{1}{N}\sum_{i=0}^{N-1-\ell}y[i]y[i+\ell] \\
& \underbrace{\frac{1}{N}\sum_{j=1}^{M_N}\sum_{i=0}^{L-\ell-1}x[i]x[i+\ell]}_{\text{signal term}} + \underbrace{\frac{1}{N}\sum_{i=0}^{N-1}\varepsilon[i]\varepsilon[i+\ell]}_{\text{noise term}},
\end{split}
\end{equation}
where the cross terms between the signal and the noise almost surely vanish in the limit. 

We treat the signal and noise terms separately. We first break the signal term into $M_N$ different sums, each contains one copy of the signal, and get
\begin{equation} \label{eq:2nd_moment_signal_term}
\frac{1}{N}\sum_{j=1}^{M_N}\sum_{i=0}^{L-\ell-1}x[i]x[i+\ell] = \frac{M_NL}{N}\frac{1}{L}\sum_{i=0}^{L-\ell-1}x[i]x[i+\ell] = \gamma a_x^2[\ell].
\end{equation}
Similarly, for $\ell\neq 0$, we can break the noise term into a sum of independent terms  \TODO{to recheck indicies}
\begin{equation}
\frac{1}{N}\sum_{i=0}^{N-1-\ell} \varepsilon[i]\varepsilon[i+\ell] = \frac{1}{\ell}\sum_{i=0}^{\ell-1}\frac{1}{N/\ell}\sum_{j=0}^{N/\ell -1} \varepsilon[j\ell + i] \varepsilon[(j+1)\ell + i].
\end{equation}
Each term of $\frac{1}{N/\ell}\sum_{j=0}^{N/\ell -1} \varepsilon[j\ell + i] \varepsilon[(j+1)\ell + i]$ is an average of $N/\ell$ independent terms with expectation zero, and thus converges to zero almost surely as $N\to\infty$.
If $\ell=0$, 
\begin{equation}
\frac{1}{N}\sum_{i=0}^{N-1} \varepsilon[i]^2 \xrightarrow{a.s.} \sigma^2.
\end{equation}

We are now moving to the third-order autocorrelation. Let us fix $\ell_1\geq\ell_2$ and recall that 
\begin{equation*}
a_y^3[\ell_1,\ell_2] = \sum_{i=0}^{N-1-\ell_1} y[i]y[i+\ell_1]y[i+\ell_2]. 
\end{equation*}
Writing explicitly in terms of signal and noise, the sum can be broken into eight partial quantities. The first contains only signal terms (does not see noise) and converges to $\gamma a_x^3$ from the same reasons as~\eqref{eq:2nd_moment_signal_term}. Three other partial sums contain the product of two signal entries and one noise term. Since the noise is independent of the signal and has zero mean, these terms go to zero almost surely.

We next analyze the contribution of the term composed of triple products of noise terms. For $\ell_1\neq 0$, this sum can be formulate as follows:
\begin{equation*}
\sum_{i=0}^{N-1-\ell_1} \varepsilon[i]\varepsilon[i+\ell_1]\varepsilon[i+\ell_2] = \frac{1}{\ell_1}\sum_{i=0}^{\ell_1-1}\frac{1}{N/\ell_1}\sum_{j=0}^{N/\ell_1 -1 }\varepsilon[j\ell_1+i]\varepsilon[(j+1)\ell_1+i]\varepsilon[j\ell_1+i+\ell_2].
\end{equation*}
For each fixed $i$, we sum of over $N/\ell_1$ independent variables that goes to zero almost surely. For $\ell_1=\ell_2=0$, we get a some of $N$ independent variables, each one is a triple product of Gaussian variables with zero mean and therefore has zero expectation. 

To complete the analysis, we consider the three terms composed of the product of two noise terms and one signal entry. Most of these terms converge to zero almost surely because of interdependency between the noise entries. For $\ell_1=0, \ell_2=0$ and $\ell_1=\ell_2$,  a simple computation shows that the sum converges to $\gamma\sigma^2a_x^1$; c.f.~\cite{boumal2017heterogeneous}.



\section{Proof of Proposition~\ref{prop:gamma_sigma}} \label{sec:proof_prop_gamma_sigma}

We aim to prove that one can estimate both $\sigma$ and $\beta = M/N$ from the observed first three moments.
Using observed (measured) quantities, we construct two quadratic equations of $\beta$, independent of $\sigma$.
Then, we show that these equations are independent and therefore $M$ is uniquely defined. 
Given $\beta$, we can estimate $\sigma$ using Proposition~\ref{prop:gamma}.
Throughout the proof, it is important to distinguish between observed and unobserved values. 
to this end, we denote the observed values by $E_i$ or $a_y^1,a_y^2,a_y^3$ and $F_i$ for functions of the signal's autocorrelations. 


Recall that $a_y^1 = \beta(\one^Tx)$ and  
and $a_y^2[0] = \beta(\|x\|^2+\sigma^2)$, where $\one\in\RL$ stands for vector of ones. Taking the product:
\begin{equation}\label{eq:E1}
\begin{split}
E_1 &:= (\beta(\one^Tx))(\beta(\|x\|^2+\sigma^2)) \\
& = \sigma^2a_y^1 + \beta^2F_1,
 \end{split}
\end{equation}
where $F_1 := L\left(a_x^3[0,0] + 2\sum_{j=1}^{L-1}(a_x^3[j,j]\right)$. 
The terms of $F_1$ can be also estimated from $a_y^3$ with bias:
\begin{equation} \label{eq:E2}
E_2:= \beta F_1 + (2L+1)\sigma^2a_y^1.
\end{equation}
Therefore, from~\eqref{eq:E1} and~\eqref{eq:E2} we get
\begin{equation} \label{eq:E12}
E_2\beta -(2L+1)\sigma^2\beta a_y^1 = E_1-\sigma^2a_y^1.
\end{equation}
Let $a_y^2:=\sum_{j=0}^{L-1}a_y^2[j]$ and recall from Proposition~\ref{prop:gamma}
\begin{equation} \label{eq:sigma2}
\sigma^2 = a_y^2 - (a^1_y)^2/\beta. 
\end{equation} 
Plugging into~\eqref{eq:E12} and rearranging we get 
\begin{equation} \label{eq:quad1}
\mathcal{A}\beta^2 + \mathcal{B}\beta + \mathcal{C} = 0,
\end{equation}
where 
\begin{align*}
\mathcal{A} &= E_2 - (2L+1)a_y^1a_y^2, \\ 
\mathcal{B} &= -E_1 + (2L+1)(a_y^1)^3 + a_y^1a_y^2  , \\
\mathcal{C} &= -(a_y^1)^3.
\end{align*}
Importantly, these coefficients are are observable quantities. 

We are now proceeding to derive the second quadratic equation. We notice that 
\begin{equation} \label{eq:E3}
E_3  = (a_y^1)^3 = \beta^3 (\one ^Tx)^3   = \beta^3 F_2,
\end{equation}
where \TODO{to verify}
\begin{equation*}
F_2 =  a_x^3[0,0] + 3\sum_{i=1}^{L-1}a_x^3[i,i] + 3\sum_{i=1}^{L-1}a_x^3[0,i] + 6\sum_{1\leq i < j\leq L-1}a_x^3[i,j].
\end{equation*}
On the other hand, from $a_y^3$ we can directly estimate $F_2$ up to scale and bias
\begin{equation} \label{eq:E4}
E_4 = \beta F_2 + (6L-3)\sigma^2a_y^1.
\end{equation}
Taking the ratio:
\begin{equation*} 
\frac{E_4}{E_3} = \frac{1}{\beta^2} + \frac{(6L-3)\sigma^2a_y^1}{E_3}, 
\end{equation*}
we conclude:
\begin{equation*}
\sigma^2 = \frac{E_4}{a_y^1(6L-3)}  - \frac{E_3}{\beta^2a_y^1(6L-3)}.
\end{equation*}
Using~\eqref{eq:sigma2} and rearranging we get the second quadratic:
\begin{equation} \label{eq:quad2}
\mathcal{D}\beta^2 + \mathcal{E}\beta + \mathcal{F} = 0,
\end{equation}
where
\begin{align*}
\mathcal{D} &= a_y^2 - \frac{E_4}{a_y^1(6L-3)}, \\ 
\mathcal{E} &= -(a_y^1)^2, \\
\mathcal{F} &= \frac{E_3}{a_y^1(6L-3)}.
\end{align*}

To complete the proof, we need to show that the two quadratic equations~\eqref{eq:quad1} and~\eqref{eq:quad2} are independent. To this end, it is enough to show that the ratio between the coefficients is not the same. 
From~\eqref{eq:quad1} and~\eqref{eq:E1}, we have 
\begin{equation*}
\frac{\mathcal{B}}{\mathcal{C}} = \frac{-E_1 + (2L+1)(a_y^1)^3 + a_y^1a_y^2}{-(a_y^1)^3} = \frac{a_y^2[0] - (2L+1)(a_y^1)^2 - a_y^2}{(a_y^1)^2}.
\end{equation*}
In addition, using~\eqref{eq:E3}
\begin{equation*}
\frac{\mathcal{E}}{\mathcal{F}} = -\frac{(6L-3)(a_y^1)^3}{E3} = 3 - 6L . 
\end{equation*}

Now, suppose that the quadratics are dependent. Then, $\frac{\mathcal{B}}{\mathcal{C}} =\frac{\mathcal{E}}{\mathcal{F}} $, or, 	
\begin{equation*}
a_y^2[0] - (2L+1)(a_y^1)^2 - a_y^2 = (a_y^1)^2(3-6L)
\end{equation*}
Rearranging the equation and writing in terms of $x$ we get 
\begin{equation} \label{eq:cond}
4(L-1)\beta (a_x^1)^2  - \sum_{i=1}^{L-1} a_x^2[i] = 0.
\end{equation}	
For generic $x$,  this polynomial equation is not satisfied. Therefore,  the equations are independent. 
More than that, for any nonzero $x$, $(a_x^1)^2 >\sum_{i=1}^{L-1} a_x^2[i]$. Therefore, if $4(L-1)\beta \geq 1$, or,
\begin{equation*}
\beta \geq \frac{1}{4(L-1)},
\end{equation*}
the condition~\eqref{eq:cond} cannot be satisfied for any signal. 

\end{document}

