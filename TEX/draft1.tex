\documentclass[english,11pt]{article}

\pdfoutput=1

\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{verbatim}
\usepackage{float}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
%\usepackage{multirow}
\usepackage{color}
\usepackage{url}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{mathtools} 
\usepackage[margin=1.2in]{geometry}

\newcommand{\TODO}[1]{{\color{red}{[#1]}}}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
%\numberwithin{figure}{section}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}[section]
\theoremstyle{definition}
\newtheorem{defn}[thm]{\protect\definitionname}
\theoremstyle{remark}
\newtheorem{claim}[thm]{\protect\claimname}
\theoremstyle{plain}
\newtheorem{lem}[thm]{\protect\lemmaname}

\newtheorem*{lem*}{Lemma}

\theoremstyle{remark}
\newtheorem{rem}[thm]{\protect\remarkname}
\theoremstyle{plain}
\newtheorem{corollary}[thm]{\protect\corollaryname}
\theoremstyle{plain}
\newtheorem{proposition}[thm]{\protect\propositionname}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\usepackage{slashbox}

\usepackage{babel}
\providecommand{\claimname}{Claim}
\providecommand{\definitionname}{Definition}
\providecommand{\lemmaname}{Lemma}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}
\providecommand{\corollaryname}{Corollary}
\providecommand{\propositionname}{Proposition}


\newcommand{\reals}{\mathbb{R}}
\newcommand{\RL}{\mathbb{R}^L}
\newcommand{\CL}{\mathbb{C}^L}
\newcommand{\RN}{\mathbb{R}^N}
\newcommand{\RNN}{\mathbb{R}^{N\times N}}
\newcommand{\CNN}{\mathbb{C}^{N\times N}}
\newcommand{\inner}[1]{\left\langle {#1} \right\rangle}
\newcommand{\hx}{\hat{x}} 
\newcommand{\one}{\mathbf{1}} 
\newcommand{\SNR}{{\textsf{SNR}}} 

\begin{document}

\title{Estimation below the detection limit}


\author{Tamir Bendory}
\maketitle

\begin{abstract}
	Here comes the abstract
\end{abstract}

\section{Introduction}

In this paper, we consider the problem of estimating a set of signals $x_1,\ldots,x_K$ from their multiple occurrences in unknown, random, locations in a data sequence $y$. The data may also contain background information -- independent of the signals -- which is modeled as noise.
For one-dimensional signals, the data can be thought of as a long time series and the signals as repetitive short events. For two-dimensional signals, $y$ presents a big image, containing many smaller images.  
The problem is then to estimate the signals $x_1,\ldots x_K$ from $y$. This
model appears, in different noise levels, in many applications, including spike sorting~\cite{lewicki1998review}, passive radar~\cite{gogineni2017passive} and system identification~\cite{ljung1998system}.
In Section~\ref{sec:model} we provide a precise mathematical formulation of the model and the estimation problem.

If the noise level is negligible, estimating the signals is easy. 
In this scenario, standard detection and clustering algorithms can produce multiple copies of each signal that can be then averaged. 
Even in higher noise level regimes, clever methods based on template matching, such as those used in structural biology~\cite{heimowitz2018apple} and radar~\cite{gogineni2017passive}, may work. \TODO{Do we want to add another 1D example to demonstare the problem, similarly to Figure 1.1 in the bispectrum paper?}
However, in the low signal--to--noise (\SNR) regime, detection of individual signal occurrences is impossible as explained in Section~\ref{sec:model}. Figure~\ref{fig:example} illustrates the problem in different noise levels and one underlying signal ($K=1$). 
Figure~\ref{fig:signal2D_clean} shows a $21\times 21$ image, which is a downsampled version of a projection of TKTK, taken from \textsf{ASPIRE} package~\cite{aspire} \TODO{The image is taken from the example folder}. 
Figure~\ref{fig:data2D_clean} shows a excerpt of a big image (the data image) that contains many repetitions of the projection. In Figures~\ref{fig:data2D_noisy_02} and~\ref{fig:data2D_noisy_1}, the same excerpt is shown with the addition of i.i.d.\ Gaussian noise with standard deviations of $\sigma=0.2$ and $\sigma=1$, respectively.  Figures~\ref{fig:signal2D_LS} and~\ref{fig:signal2D_LS_inter} show estimates of the projection from noisy data with $\sigma=1$. These examples demonstrate that our method can work even if the data may seem as a pure noise. In Section~\ref{sec:numerics} we provide the details of this experiment and show more corroborating experiments. 

In this work we focus on the low $\SNR$ regime. In order to estimate the signal, we use autocorrelation analysis of the data. 
In a nutshell, the method consists of two stages. First, we estimate a mix of the low--order autocorrelation functions of the signals from the data. These quantities can be estimated, to any desired accuracy, if the signals appear enough times in the measurement, without the need to detect individual occurrences. Then, the signals are estimated from the mixed autocorrelations using a non-convex least-squares.
In Section~\ref{sec:autocorrelation}, we elaborate on the technique and prove some of its properties.
Interestingly, expectation-maximization (EM) --  a popular framework for similar estimation problems, such as Gaussian mixture models and multireference alignment -- is intractable for this problem. Even if the number of signal occurrence $M$ is known and $K=1$, at each iteration 
EM needs to assign probabilities to each possible combination of  $M$ copies of the current signal estimation in the measurement.



\begin{figure}

\advance\leftskip-2cm
\advance\rightskip -2cm

%	\centering
\begin{subfigure}{.6\textwidth}
	\centering
	\includegraphics[scale=0.6]{data2D_clean}
	\vspace*{-20pt}
\caption{}
\label{fig:data2D_clean}
	\end{subfigure}%
	\begin{subfigure}{.6\textwidth}
	\centering
	\includegraphics[scale=0.6]{signal2D_clean}
	\vspace*{-20pt}
	\caption{}
	\label{fig:signal2D_clean}
\end{subfigure}%


	\begin{subfigure}{.6\textwidth}
	\centering
	\includegraphics[scale=0.6]{data2D_noisy_02}
	\vspace*{-20pt}
	\caption{}
	\label{fig:data2D_noisy_02}
\end{subfigure}
\begin{subfigure}{.6\textwidth}
	\centering
	\includegraphics[scale=0.6]{signal2D_LS_all_images}
	\vspace*{-20pt}
	\caption{}
	\label{fig:signal2D_LS}
\end{subfigure}


\begin{subfigure}{.6\textwidth}
\centering
\includegraphics[scale=0.6]{data2D_noisy_1}
\vspace*{-20pt}
\caption{}
\label{fig:data2D_noisy_1}
\end{subfigure}
\begin{subfigure}{.6\textwidth}
	\centering
	\includegraphics[scale=0.6]{signal2D_LS_intermediate}
%	\vspace*{-20pt}
	\caption{}
	\label{fig:signal2D_LS_inter}
\end{subfigure}

\caption{Figure (a) shows an excerpt of 2D data with multiple signal occurrences and no noise. Each small image is the $21\times 21$ image shown in Figure (b). Figures (c) and (e) show the same data, now contaminated with i.i.d.\ Gaussian noise with $\sigma=0.2$ and $\sigma=1$, respectively. Figures (d) and (f) show estimates of the image (b) from data with noise level $\sigma=1$ (as in Figure (e)). The signal appeared $M = 561\times 10^6$ and $M = 110\times 10^6$ times and the normalized recovery error is  $0.078$ and $0.135$, respectively.}
%	we used $71\times10^4$
% (d) 710K micrographs, $M=561Mega$, error  =  (e)The same data as in Figure~\ref{fig:signal2D_clean} contaminated with i.i.d./ Gaussian noise with $\sigma=0.2$ (f) Recovery with 140K micrographs and $M = 110Mega$, error = 0.135}
\label{fig:example}
\end{figure}



This work is primely motivated by cryo-electron microscopy (cryo--EM), which is an innovative  single particle reconstruction technology. The acquired data in a cryo-EM experiment is contaminated with high noise levels. Therefore, any molecule reconstruction algorithm must take  the challenging $\SNR$ level into account.  In the last part of this manuscript, we draw connections with the estimation problem under consideration and the cryo--EM problem.


\section{Model}  \label{sec:model}

Let $x_1,\ldots,x_K\in\RL$ be the sought signals and let $y\in\RN$ be the data. For each $x_i$, we associate a binary signal  $s_i\in\{0,1\}^N$, referred to as the \emph{support signal} and let $s = \sum_{i=1}^Ks_i$. The nonzero values of $s_i$ indicates the locations of $x_i$ in $y$. If $s_i[n]=1$, then $y[n+j] = x_i[n+j]+\varepsilon[n+j]$ for $j=0,\ldots,L-1$.
We denote the cardinality of $s_i$ by $M_i$  and $M = \sum_{i=1}^{K}M_i$. Neither the $M_i$'s nor $M$ is assumed to be known.


The support signals are generated by the following generative model. The signal $s$ is initialized with zeros.
First, an index $i_1$ is drawn uniformly from $\{1,\ldots,N\}$ and we set $s[i_1]=1$. A second index $i_2$ is then drawn uniformly from $\{1,\ldots,N\}$. If $\vert i_2-i_1\vert \geq L$,
then we set $s[i_2]=1$, otherwise we keep $s[i_2]=0$ and draw a new index  from uniform distribution. We then proceed adding  nonzero entries to $s$,  while keeping $L$  entries separation until some halting criterion is obtained. 
We note that if the support is sparse enough, this generative model can be approximated by a simple Bernoulli process which takes the values of one and zero with probabilities  $M/N$ and $1-M/N$, respectively.
Once $s$ is determined, each one of its nonzero entries is associated with one of the $s_i$'s. 
In particular, for each $s[k]\neq 0$ we set $s_i[k] = 1$ and $s_j[k] = 0$ for $i\neq j$, where $i$ is drawn form an unknown probability over  $\{1,\ldots,K\}$. 

The simplest way to present the forward model is a mix of blind deconvolution problems between the support signals and the target signals
\begin{equation}
y = \sum_{i=1}^K x_i\ast s_i + \varepsilon,\quad \varepsilon\sim\mathcal{N}(0,\sigma^2 I).
\end{equation}
We model the background information as i.i.d.\ Gaussian noise with zero mean and $\sigma^2$ variance. 
The goal is to estimate $x_1,\ldots,x_K$ from $y$.

Blind deconvolution is a longstanding problem, arising in a variety of engineering and scientific applications, such as astronomy, communication, image deblurring, system identification and optics; see~\cite{jefferies1993restoration,shalvi1990new,ayers1988iterative,abed1997blind}, just to name a few. Clearly, the problem is ill-posed without additional information. In our case, the prior information is that $s$ is a binary, sparse, signal. 
Other settings of blind deconvolution problems have been analyzed recently under different settings, see for instance~\cite{ahmed2014blind,li2016identifiability,li2016rapid,ling2015self,ling2017blind,chi2016guaranteed}
where the focus is on high $\SNR$ regimes.


An important feature of the problem under consideration is that while both $x_i$'s and $s_i$'s are unknown, the goal is merely to estimate the $x_i$'s. The  $s_i$'s  are referred to as \emph{nuisance  variables}. Indeed, in many blind deconvolution applications the goal is merely to estimate one of the unknown signals. For instance, in image deblurring, both the blurring
kernel and the high-resolution image are unknown, but the prime goal is only
to sharpen the image.
If $x$ is known and $K=1$, then  $s$ can be estimated by linear programming  in the high $\SNR$ regime~\cite{de2012exact,duval2015exact,bendory2016robust,bendory2017robust,bernstein2017deconvolution}. However, in the low $\SNR$ regime, estimating $s$ is impossible. To see that, suppose that an oracle provides us $M$ windows of length $W>L$, each contains one copy of $x$. Put it differently, we get a series of windows, each one contains a signal at unknown location. 
Estimating the first entry of the signal within each  window is an easier problem than detecting the support of $s$. However, even this problem -- called alignment or synchronization -- is impossible in the low $\SNR$ regime.
For instance, the variance of any estimator is, at best, proportional to $\sigma^2$ and independent of the number of windows, even if $x$ is known~\cite{aguerrebere2016fundamental}. Therefore, we conclude that that detecting the nonzero values of $s$ is impossible in low \SNR. 
In the next section we show that if $M$ is large enough, then estimating the signals is possible, in any $\SNR$ level and to any accuracy, although we cannot detect their occurrences in $y$. 



\section{Autocorrelation analysis}   \label{sec:autocorrelation}

Our method for estimating the signals relies on two pillars. 
First, we use the autocorrelation functions of the data to estimate a mix (i.e., linear combination) of the $K$ signals autocorrelations. The mixed autocorrelation can be estimated to any accuracy, in any $\SNR$ level, if $M$ is large enough and the separation condition on the support is held. Then, we use least-squares (LS) estimator to estimate the signals from their mixed autocorrelation. From pedagogical reasons, we start with the second stage of recovering signals from their autocorrelations. In this section we mainly discuss the uniqueness of the mapping from the autocorrelation to the signals. Concrete algorithms will be discussed in the next section.

For the purpose of this paper, we need the first three (aperiodic) autocorrelation functions of a signal $z\in\mathbb{R}^W$, defined as
\begin{align} \label{eq:autocorrelations}
a_z^1 &= \sum_{i=0}^{W-1} z[i], \nonumber\\
a_z^2[\ell] &= \sum_{i=0}^{W-1-\ell} z[i]z[i+\ell], \nonumber\\
a_z^3[\ell_1,\ell_2] &= \sum_{i=0}^{W-1-\max\{\ell_1,\ell_2\}} z[i]z[i+\ell_1]z[i+\ell_2]. 
\end{align}
Note that the autocorrelation functions are symmetric so that $a_z^2[\ell] = a_z^2[-\ell]$ and $a_z^3[\ell_1,\ell_2] = a_z^3[-\ell_1,-\ell_2]$. 

A one-dimensional signal is determined uniquely by its third-order auto-correlation:
\begin{proposition}
	Suppose that $z[0]$ and $z[L-1]$ are non-zeros. Then, a signal $z\in\mathbb{R}^W$ is determined uniquely from  $a_z^2$ and $a_z^3$.
	\begin{proof}
		From the second-order autocorrelation, we ge
		\begin{equation*}
		a_z^2[L-1] = z[0]z[L-1]\neq 0.
		\end{equation*}
		Then, from the third-order autocorrelation we can compute $z[k]$ for all $k=0,\ldots L-1$ by
		\begin{eqnarray}
		a_z^3[k,L-1] = z[0]z[k]z[L-1].
		\end{eqnarray}
	\end{proof}
\end{proposition}
We note that the length of the signal can be easily derived from the second-order autocorrelation of the signal. Therefore, the assumption that $z[0]$ and $z[L-1]$ is met in practice. We also note that for one-dimensional signals, the second-order autocorrelation does not determined the signals uniquely~\cite{beinert2015ambiguities,bendory2017fourier}. This is not the case for dimensions greater than one, in which almost all signals are determined uniquely from their aperiodic autocorrelations, up to sign (phase in the complex case) and reflection through the origin (with conjugation in the complex case)~\cite{hayes1982reconstruction,hayes1982reducible,bendory2017fourier}. The sign ambiguity can be resolved by the mean of the signal if it is not zero. However,  in order to determine the reflection symmetry ($Z_2$ symmetry), one needs to use the third moment.

The heterogeneous case $K>1$ was explored for periodic autocorrelation functions. The aperiodic autocorrelations are the periodic autocorrelations of signals padded by zeros. Therefore, the following results hold for our setup as well  \TODO{This part is wrong -- Email correspondence with Alex}. In~\cite{bandeira2017estimation}, it was shown that a mix of $K$ third-order autocorrelations determine  a finite list of $K$ generic signals when 
\begin{itemize}
	\item $K=2$ when $L>1$,
	\item $K=3$ when $L>12$,
	\item $K=4$ when $L>18$,
	\item $5\leq K\leq 15$ when $L\geq 6K-5$.	
\end{itemize}
The authors conjecture that for $K>15$, $L\geq 6K-5$ is enough. Recently, Joe told use that he has proven that there is unique mapping in the same regime.  
This also holds for arbitrary weights $\gamma_i$ which are rational fractions.

\TODO{Where we should refer to Gianakis's paper?}

We are moving forward to discuss the second question of how to estimate the autocorrelation of the signals from the data. 
The analysis is conducted in the asymptomatic regime where $M_1,\ldots,M_K,N\to\infty$. We define the ratios 
\begin{equation}
\gamma_i = \frac{M_i L}{N},
\end{equation}
and $\gamma = \sum_{i=1}^K\gamma_i$.
Under the separation condition, we have $\gamma\leq\frac{L}{2L-1}\approx 1/2$.
The main insight is that if $s$ satisfies the separation condition, then the first $L$ entries of the data autocorrelations converge to a $\gamma$ scaled version of a mix of the signals autocorrelations:
\begin{align}
\lim_{N\to\infty} a_y^1 &= \sum_{i=1}^K\gamma_i a_{x_i}^1, \\
\lim_{N\to\infty} a_y^2[\ell] &= \sum_{i=1}^K\gamma_i a_{x_i}^2[\ell] +\sigma^2\delta[\ell],\\
\lim_{N\to\infty} a_y^3[\ell_1,\ell_2] &= \sum_{i=1}^K\gamma_i a_{x_i}^3[\ell_1,\ell_2] + \sigma^2\left(\sum_{i=1}^K\gamma_i a_{x_i}^1\right)(\delta[\ell_1,0]+\delta[0,\ell_2]+\delta[\ell_1,\ell_2]),
\end{align}
for $\ell,\ell_1,\ell_2=0,\ldots L-1$.
These relations are proved in Appendix~\ref{sec:autocorrelation_computation}. The analysis is similar to~\cite{bendory2017bispectrum,boumal2017heterogeneous}, yet a special caution should be taken with the noise dependencies. This means that given $M_1,\ldots,M_K$ and $\sigma^2$ and $K$ does not exceed the limit for uniqueness, the one can estimate the signals from the third-order autocorrelation of the data. In other words, the signals can be estimated in asymptotic estimation rate of $\sigma^6/N$.


If the noise level $\sigma^2$ is known, then for $K=1$, one can estimate $M$ from only the first two moments, namely, with estimation rate of $\sigma^4/N$.
\begin{proposition} \label{prop:gamma}
	Let $K=1$. Then, 
	\begin{equation*}
	\frac{M}{N} = \frac{(a^1_y)^2}{\sum_{j=0}^{L-1}a_y^2[j]-\sigma^2}.
	\end{equation*}
	\begin{proof}
The relation is proved by plugging the definitions of the signal autocorrelations into the right-hand side of the equation~\eqref{eq:autocorrelations}. 	
\end{proof}
\end{proposition}


\begin{proposition} \label{prop:gamma_sigma}
	It is possible to estimate $M$ and $\sigma$ from the third moment.
	\begin{proof}
		See Appendix~\ref{sec:proof_prop_gamma_sigma}.
	\end{proof}
\end{proposition}
If we assume that $s$ is sparse, then by ignoring the separation condition, one can describe the generative process of $s$ as a Bernoulli process with parameter $M/N$. Therefore, in low \SNR  one can estimate the parameter that control this statistical process, although cannot estimate individual entries.

Estimating $\sigma$ and $M$? 


\section{Numerical experiments}   \label{sec:numerics}

\subsection{One-dimensional experiments}


\subsection{Two-dimensional experiments}

Solving RRR, and then 200 LS iteration on window of size $W$ and 500 iterations on window of size $L$.

\section{Conclusion} 
Here we conclude the paper: cryo -- EM, structured background, without separation, heterogeneity
In particular, in cryo--EM, multiple projections of a molecule (signal), taken
from unknown viewing directions, are recorded on two-dimensional detector
array. Current algorithms try to detect these projections in a low SNR regime
and then use them for the reconstruction process. Since the
Then, the reconstruction problem is to detect these projection, and then use
them to estimat
The data acquired in both technologies is composed of multiple projections
of a molecule (the signal), taken from unknown viewing direction, and drawn in
high noise level. The reconstitution problem is then to estimate the molecule
from this data.

A word on the sample complexity

\bibliographystyle{plain}
\bibliography{ref}



\appendix

\section{Autocorrelation estimations} \label{sec:autocorrelation_computation}
 
To analyze the asymptotic behavior of the data autocorrelation functions, we consider one signal $K=1$. The extension to $K>1$ is straightforward by averaging the contributions of all signal with the appropriates weights, see~\cite{boumal2017heterogeneous}. 

Let us define
\begin{equation}
\gamma = \lim_{N\to\infty} \frac{M_NL}{N}<1.
\end{equation}
By assuming $M_N=\Omega(N)$, we also have $\gamma>0$.
We start by considering the first autocorrelation of the data
\begin{equation}
a_y^1 = \sum_{i=0}^{N-1} y[i] = \frac{1}{N/L}\sum_{j=0}^{M_N-1}\frac{1}{L}\sum_{i=0}^{L-1}x[i] + \underbrace{\frac{1}{N}\sum_{i=0}^{N-1}\varepsilon[i]}_{\text{noise term}} \xrightarrow{a.s.}\gamma a_x^1,
\end{equation}
where the noise term converges to zero almost surely (a.s.) by the law of large numbers.

We proceed with the second moment for fixed $\ell\in[0,\ldots,L-1]$. Then, we can compute, 
\begin{equation}
\begin{split}
a_y^2[\ell] & = \frac{1}{N}\sum_{i=0}^{N-1-\ell}y[i]y[i+\ell] \\
& \underbrace{\frac{1}{N}\sum_{j=1}^{M_N}\sum_{i=0}^{L-\ell-1}x[i]x[i+\ell]}_{\text{signal term}} + \underbrace{\frac{1}{N}\sum_{i=0}^{N-1}\varepsilon[i]\varepsilon[i+\ell]}_{\text{noise term}},
\end{split}
\end{equation}
where the cross terms between the signal and the noise almost surely vanish in the limit. 

We treat the signal and noise terms separately. We first break the signal term into $M_N$ different sums, each contains one copy of the signal, and get
\begin{equation} \label{eq:2nd_moment_signal_term}
\frac{1}{N}\sum_{j=1}^{M_N}\sum_{i=0}^{L-\ell-1}x[i]x[i+\ell] = \frac{M_NL}{N}\frac{1}{L}\sum_{i=0}^{L-\ell-1}x[i]x[i+\ell] = \gamma a_x^2[\ell].
\end{equation}
Similarly, for $\ell\neq 0$, we can break the noise term into a sum of independent terms 
\begin{equation}
\frac{1}{N}\sum_{i=0}^{N-1-\ell} \varepsilon[i]\varepsilon[i+\ell] = \frac{1}{\ell}\sum_{i=0}^{\ell-1}\frac{1}{N/\ell}\sum_{j=0}^{N/\ell -1} \varepsilon[j\ell + i] \varepsilon[(j+1)\ell + i].
\end{equation}
Each term of $\frac{1}{N/\ell}\sum_{j=0}^{N/\ell -1} \varepsilon[j\ell + i] \varepsilon[(j+1)\ell + i]$ is an average of $N/\ell$ independent terms with expectation zero, and therefore converge to zero almost surely as $N\to\infty$.
If $\ell=0$, 
\begin{equation}
\frac{1}{N}\sum_{i=0}^{N-1} \varepsilon[i]^2 \xrightarrow{a.s.} \sigma^2.
\end{equation}

We are now moving to the third-order autocorrelation. Let us fix $\ell_1\geq\ell_2$ and recall that 
\begin{equation*}
a_y^3[\ell_1,\ell_2] = \sum_{i=0}^{N-1-\ell_1} y[i]y[i+\ell_1]y[i+\ell_2]. 
\end{equation*}
Writing explicitly in terms of signal and noise, this sum can be broken into eight partial sums. The first contains only signal terms and converges to $\gamma a_x^3$ from the same reasons as~\eqref{eq:2nd_moment_signal_term}. Three other partial sums contain the product of two signal entries and one noise term. Since the noise is independent of the signal, then these terms go to zero almost surely.

We next analyze the contribution of triple product of noise terms. For $\ell_1\neq 0$, this sum can be formulate as follows:
\begin{equation*}
\sum_{i=0}^{N-1-\ell_1} \varepsilon[i]\varepsilon[i+\ell_1]\varepsilon[i+\ell_2] = \frac{1}{\ell_1}\sum_{i=0}^{\ell_1-1}\frac{1}{N/\ell_1}\sum_{j=0}^{N/\ell_1 -1 }\varepsilon[j\ell_1+i]\varepsilon[(j+1)\ell_1+i]\varepsilon[j\ell_1+i+\ell_2].
\end{equation*}
For each fixed $i$, we sum of over $N/\ell_1$ independent variables that goes to zero almost surely. For $\ell_1=\ell_2=0$, we get a some of $N$ independent variables, each is a triple product of Gaussian variables with zero mean. Therefore, it is also converges to zero. 

To complete the analysis, we consider the three terms composed of the product of two noise terms and one signal entry. Most of these terms converges to zero almost surely because of interdependency between the noise entries. For $\ell_1=0, \ell_2=0$ and $\ell_1=\ell_2$,  a simple computation shows that the sum converges to $\gamma\sigma^2a_x^1$.



\section{Proof of Proposition~\ref{prop:gamma_sigma}} \label{sec:proof_prop_gamma_sigma}

We aim to prove that one can estimate both $\sigma$ and $M$ from the observed third moment. We will construct two independent quadratic equations of $M$ which do not depend on $\sigma$. This will let us estimate $M$. Given $M$, we will estimate $\sigma$.

To ease notation, let $\beta=M/N$. Recall that $a_y^1 = \beta(\one^Tx)$ and  
 and $a_y^2[0] = \beta(\|x\|^2+\sigma^2)$, where $\one\in\RL$ stands for vector of ones. Taking the product:
\begin{equation}\label{eq:E1}
\begin{split}
E_1 &:= (\beta(\one^Tx))(\beta(\|x\|^2+\sigma^2)) \\
& = \sigma^2a_y^1 + \beta^2\tau,
 \end{split}
\end{equation}
where $\tau := L\left(\sum_{j=1}^{L-1}(a_x^3[j,0]+a_x^3[0,j] )+\sum_{j=0}^{L-1}a_x^3[0,0]\right)$. Next, we can estimate from $a_y^3$
\begin{equation} \label{eq:E2}
E_2:= \beta \tau + 3L\sigma^2a_y^1.
\end{equation}
Therefore, from~\eqref{eq:E1} and~\eqref{eq:E2} we get
\begin{equation}
E_2\beta -3L\sigma^2a_y^1 = E_1-\sigma^2a_y^1.
\end{equation}
Now, recalling from Proposition~\ref{prop:gamma}
\begin{equation} \label{eq:sigma2}
\sigma^2 = a_y^2 - (a^1_y)^2/\beta, 
\end{equation} 
where we use $a_y^2:=\sum_{j=0}^{L-1}a_y^2[j]$.
Rearranging the equations, we then get the quadratic equation
\begin{equation} \label{eq:quad1}
\mathcal{A}\beta^2 + \mathcal{B}\beta + \mathcal{C} = 0,
\end{equation}
where 
\begin{align*}
\mathcal{A} &= E_2, \\ 
\mathcal{B} &= -(E_1 + (3L-1)a_y^1a_y^2), \\
\mathcal{C} &= (3L-1)(a_y^1)^3.
\end{align*}

We are now proceeding to derive the second quadratic equation. We notice that \begin{equation} \label{eq:E3}
E_3  = (a_y^1)^3 = \beta^3 \upsilon,
\end{equation}
where \TODO{to check} $$\upsilon = a_x^3[0,0] + 3\sum_{i=1}^{L-1}a_x^3[i,i] + 3\sum_{i=1}^{L-1}(a_x^3[i,0]+a_x^3[0,i]) + 6\sum_{1\leq i \leq j\leq L-1}a_x^3[i,j].$$
On the other hand, from $a_y^3$ we can directly estimate
\begin{equation} \label{eq:E4}
E_4 = \beta\upsilon + 9L\sigma^2a_x^1.
\end{equation}
Now, by multiplying both sides by $\beta^2$ and using~\eqref{eq:E3}, we get 
\begin{equation} \label{eq:E4}
\beta^2E_4 =  E_3  + 9L\sigma^2a_x^1.
\end{equation}
Plugging~\eqref{eq:sigma2} and rearranging the equation, we get a second quadratic equation independent of $\sigma$:
\begin{equation} \label{eq:quad2}
\mathcal{D}\beta^2 + \mathcal{E}\beta + \mathcal{F} = 0,
\end{equation}
where
\begin{align*}
\mathcal{D} &= E_4 - 9La_x^1a_y^2, \\ 
\mathcal{E} &= 9La_x^1(a_y^1)^2, \\
\mathcal{F} &= -(a_y^1)^3.
\end{align*}

To complete the proof, we need to show that the two quadratic equations~\eqref{eq:quad1} and~\eqref{eq:quad2} are independent. To this end, it is enough to show that the ratio between the coefficients is not the same. 
using~\eqref{eq:E1}, we have 
\begin{equation*}
\frac{\mathcal{B}}{\mathcal{C}} = \frac{-(E_1 + (3L-1)a_y^1a_y^2)}{(3L-1)(a_y^1)^3} = -\frac{a_y^2[0]}{(3L-1)(a_y^1)^2} - \frac{a_y^2}{(a_y^1)^2}.
\end{equation*}
In addition, 
\begin{equation*}
\frac{\mathcal{E}}{\mathcal{F}} = -\frac{9La_x^1}{a_y^1}. 
\end{equation*}
Now, suppose that the quadratics are dependent. Then, $\frac{\mathcal{B}}{\mathcal{C}} =\frac{\mathcal{E}}{\mathcal{F}} $, or, 	
\begin{equation*}
\frac{a_y^2[0]}{(3L-1)(a_y^1)^2} + \frac{a_y^2}{(a_y^1)^2} = \frac{9La_x^1}{a_y^1}. 
\end{equation*}
Rearranging the equation, we get 
\begin{equation*}
 a_y^2[0] + (3L-1)(a_y^1)^2 - 9La_x^1a_y^1=0.
\end{equation*}	
For generic $x$,  this polynomial equation is not satisfied. Therefore,  the equations are independent. 

\end{document}

